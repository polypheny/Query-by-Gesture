\section{Introduction}
\label{g1:sec:introduction}  % TODO Adopt group prefix

The goal is to input relational query plans via gesture. To realize this, we implemented a bridge which parses detected gestures to a JSON String which can be interpreted in the Polypheny UI. For this each gesture is assigned to exactly one query attribute or "escape" commands to navigate in the tree. Due to this hard linked states forfeit the intuitiveness of the interaction.
\\
\\
Further, might be difficultly complicated, we thought of a drag and drop style interaction, where the mouse is replaced by the gestures of thumb and index finger. It should behave like a touchscreen. Precisely, the camera feed and the Polypheny-DB-UI are overlaid so the users sees where the fingers are and the operations can be picked by closing thumb and index finger and dragging it into the working field. Unfortunately we could not realise this, because as expected, it was way to complicated. This method would have been much more intuitive and has a better human-machine interaction (HMI).
\\
\\
The project idea itself is innovative and futuristic. The invention of the touchscreen has been the foundation of HMI without using a keyboard or a mouse. The next step would be to not even touch anything anymore, which can be realised by voice and gesture control. Alexa made already a big step into this direction. If a reliable gesture control is developed, it will only take a few steps to feel like Tony Stark, while talking to Jarvis and controlling his computer free handed and only by gestures. In this report we find out, how many more steps have to be taken to a useful and reliable gesture control.

