\section{Introduction}
\label{g1:sec:introduction}  % TODO Adopt group prefix

The goal of this project is to input relational query plans via gesture. For this, we used two existing projects of the University of Basel and connected them. The gesture recognition is done by Deepmime while the query visualization and processing are done by Polypheny. To realize our goal, we implemented a bridge that parses detected gestures to a JSON String which can be interpreted and represented in the Polypheny UI. For this, each gesture is assigned to exactly one query attribute or navigation commands to build the tree. These hard-linked states forfeit the intuitiveness of the interaction.
\\
\\
The project idea itself is innovative and futuristic. The invention of the touchscreen has been the foundation of Human-Maschine-Interaction (HMI) without using a keyboard or a mouse. The next step would be to not even touch anything anymore, which can be realised by voice and gesture control. Alexa made already a big step into this direction. If a reliable gesture control is developed, it will only take a few steps to feel like Tony Stark, while talking to Jarvis and controlling his computer free handed and only by gestures and voice control. In this report, we find out, how many more steps have to be taken to achieve a useful and reliable gesture control.

